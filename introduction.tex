\section{Introduction}
After their introduction in 2014 by Ian Goodfellow generative adversarial networks (GANs) have become the subject of an intensive research~\cite{gan}. Generative adversarial networks belong, as the name suggests, to the class of generative models. Such models try to learn the underlying probability distribution of the observed data. Once learned, this distribution can be used to generate new, realistic samples of data, which, depending on the training dataset, could be an image of a human face, a piano melody or a piece of poetry. Generative adversarial networks also introduced us to new problems, not common for discriminative models. The first one of these problems is the unstable training. GANs have proven to be very sensitive to the choice of hyper-parameters and the network architecture~\cite{wgan}. The newly proposed WassersteinGAN (WGAN) aims to relax this sensitivity by providing a new type of loss function, which allows to train a wider variety of networks and spend less time finding a perfect set of hyper-parameters~\cite{wgan}. Another problem inherent to GANs is that the quality of the data produced by a network is hard to evaluate automatically. This together with the fact that new GAN architectures emerge weekly makes it hard to compare all of them to choose the state-of-the-art one. In the case of discriminative models with labeled data, like image classification, evaluation of network performance is a straightforward task. One could simply run a network on a test dataset and count the number of correctly classified images. This is not possible with generative models, that produce some new, unseen data. The process of judging the quality of this data is hard to automate and sometimes it is easier to do it manually. Of coarse, the problem with manual approach is that it is not entirely objective and does not scale. Still, there is no standard framework for evaluating performance of different GAN architectures. While several different techniques have been proposed~\cite{gam, inception}, many researchers demonstrate their results by providing multiple samples generated by their GAN implementation~\cite{dcgan}. \\ 
\indent In this thesis I have implemented two deep convolutional generative adversarial networks (DCGANs) with both cross-entropy and Wasserstein loss functions respectively. Both these networks were trained on CelebA dataset to generate realistic human faces~\cite{celeba}. Then I tried to compare their performance by using a modification of the generative adversarial metric (GAM). Furthermore, I have investigated the function learned by the GAN discriminator trying to identify whether it really understands how to distinguish real data from the fake one. 